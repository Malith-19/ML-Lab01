# -*- coding: utf-8 -*-
"""lab 1 _190145G.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvrK47KmnFQQX0JG_BgET7PDQ_M-7q7M

## Before run
- Please change the selected_label parameter under the **Train the data with all features** heading.
- Change the correlation_threshold under the **Dropping the features using correlation threshold.** heading.

## Load the data
"""

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from google.colab import drive, files

drive.mount('/content/drive')

train_data = pd.read_csv('/content/drive/MyDrive/ML/train.csv')
train_data.head()

test_data = pd.read_csv('/content/drive/MyDrive/ML/test.csv')
test_data = test_data.iloc[:,:256]
test_data.head()

train_data.shape

"""Checking for any null values in the data set."""

train_data.isnull().sum()

"""### Droping the data for missing values."""

train_data = train_data.dropna()
train_data.isnull().sum()

"""## Train the data with all features."""

X_train = train_data.drop(['label_1', 'label_2', 'label_3', 'label_4'], axis=1)
X_train.head()

selected_label = 'label_3' #@param ["label_1", "label_2", "label_3", "label_4"]
y_train = train_data[selected_label]
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_train

"""### Create and Train the XGBoost Model."""

# Create an XGBoost classifier (or regressor for regression tasks)
model = xgb.XGBClassifier(verbosity=2, tree_method="gpu_hist")  # You can specify hyperparameters here

# Train the model on the training data
model.fit(X_train, y_train)

"""### Loading the valid data set."""

valid_data = pd.read_csv("/content/drive/MyDrive/ML/valid.csv")
valid_data.isnull().sum()

"""### Droping data for missing values in the valid dataset."""

valid_data = valid_data.dropna()
valid_data.isnull().sum()

y_test = valid_data[selected_label]
valid_data = valid_data.iloc[:,:256]
valid_data.head()

"""### Make predictions on the valid set"""

y_pred = model.predict(valid_data)
y_pred = le.inverse_transform(y_pred)
y_pred

"""### Make predictions on the test set"""

y_pred_test = model.predict(test_data)
y_pred_test = le.inverse_transform(y_pred_test)
y_pred_test

"""### Evaluate the model."""

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

"""### Adding the predicted values to output dataframe."""

output_dataframe = pd.DataFrame()
output_dataframe['Predicted labels before feature engineering'] = y_pred_test
output_dataframe.head()

"""## Reducing features using correlation analysis.

### Calculation correlation matrix
"""

correlation_matrix = train_data.corr()
correlation_matrix

# Calculate the absolute correlations with the target variable
correlations_with_target = correlation_matrix[selected_label].abs()
correlations_with_target.describe()

# Sorting the corelations.
sorted_correlations = correlations_with_target.sort_values(ascending=False)
sorted_correlations

"""### Droping the features using correlation threshold."""

correlation_threshold = 0.2
low_correlation_mask = (sorted_correlations < correlation_threshold)

# Removing the labels from the correlation mask since value data set don't have them.
labels = ['label_1', 'label_2', 'label_3', 'label_4']
low_correlation_mask_without_labels = low_correlation_mask.drop(labels, axis=0)
low_correlation_mask_without_labels

x_train_filtered = X_train.drop(columns=X_train.columns[low_correlation_mask_without_labels])
x_train_filtered.head()

"""### Create and Train the XGBoost Model."""

# Create an XGBoost classifier (or regressor for regression tasks)
model = xgb.XGBClassifier(verbosity=2, tree_method="gpu_hist")  # You can specify hyperparameters here

# Train the model on the training data
model.fit(x_train_filtered, y_train)

"""### Make predictions on the valid set using the XGBoost model."""

valid_data_filtered = valid_data.drop(columns=valid_data.columns[low_correlation_mask_without_labels])

y_pred = model.predict(valid_data_filtered)
y_pred = le.inverse_transform(y_pred)
y_pred

"""### Make predictions on the test data set using the XGBoost Model."""

test_data_filtered = test_data.drop(columns=test_data.columns[low_correlation_mask_without_labels])

y_pred_test = model.predict(test_data_filtered)
y_pred_test = le.inverse_transform(y_pred_test)
y_pred_test

"""### Evaluate the XGBoost Model."""

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

"""### Adding predicted values to output dataframe after feature engineering."""

output_dataframe['Predicted labels after feature engineering'] = y_pred_test
output_dataframe['No of new features'] = x_train_filtered.shape[1]
output_dataframe = pd.concat([output_dataframe.reset_index(drop=True), test_data_filtered.reset_index(drop=True)],
                             axis=1)
output_dataframe

"""### Exporting the dataframe to csv."""

filename = '190145G_'+selected_label+'.csv'

# Adding null columns
start_point = output_dataframe['No of new features'][0]
output_dataframe = output_dataframe.assign(**{f'New_Column_{i}': None for i in range(start_point, 256)})

# Renaming the columns as needed.
new_column_names = ['Predicted labels before feature engineering', 'Predicted labels after feature engineering', 'No of new features']
for i in range(1, 257):
  new_column_names.append('new_feature_'+str(i))

output_dataframe.columns = new_column_names
output_dataframe.to_csv(filename)
files.download(filename)

"""### Create and Train the SVC Model."""

svc_model = SVC(kernel='linear', C=1.0, random_state=42)
svc_model.fit(x_train_filtered, y_train)

"""### Make predictions on the test set using the SVC Model."""

y_pred = svc_model.predict(valid_data_filtered)
y_pred = le.inverse_transform(y_pred)
y_pred

"""### Evaluate the SVC Model."""

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.2f}')

"""### Conculusion
#### Label 1
- After reducing the features to 176 still we can achieve the **0.94** accuracy using the XGBoost model with the correlation threshold 0.05.
- Since the filtering already shown some performance I tried the dataset on the SVC Model and it gives the **0.999** accuracy.
- SVC model is more suited for this dataset.

#### Label 2
- Reduced the features to 127 and it has **0.93** accuracy on XGBoost model with the correlation threshold 0.05.

#### Label 3
- Reduced the features to 77 and it has **1.00** accuracy on XGBoost model with the correlation threshold 0.2.

#### Label 4
- Reduced the features to 117 and it has **0.94** accuracy on XGBoost model with the correlation threshold 0.05.

"""